{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SIPI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIPI dataset was preprocessed using prepare_dataset notebook, so these three folders already contain Detritus/Non-Detritus images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetName = 'DatasetSIPI'\n",
    "\n",
    "train_dir = DatasetName+'/train'\n",
    "validation_dir =  DatasetName+'/val'\n",
    "test_dir = DatasetName+'/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets are loaded using keras preprocessing method *image_dataset_from_directory*. Both the batch size and the image size hyperparameters where tested using different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230883 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 20:21:20.327158: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-13 20:21:20.425226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-13 20:21:20.426248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:81:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-13 20:21:20.426278: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-13 20:21:20.756252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-13 20:21:20.756345: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-13 20:21:20.913017: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-13 20:21:21.030531: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-13 20:21:21.031941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.1/lib64\n",
      "2021-07-13 20:21:21.082259: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-13 20:21:21.098441: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-13 20:21:21.098488: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-07-13 20:21:21.112821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-13 20:21:21.116534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-13 20:21:21.116568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49474 files belonging to 2 classes.\n",
      "Found 49475 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamers to be used in all models\n",
    "base_learning_rate = 0.0001\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "TRAINING_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MobileNet model is loaded and modified so it can be used to predict Detritus images.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 20:22:13.935578: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-13 20:22:13.954232: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2100090000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 5, 5, 1280)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "mobilenet_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "densenet_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.vgg19.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "vgg_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "inception_resnet_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    print(\"Training model, epochs: \", epochs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_dataset)\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    training_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_dataset)\n",
    "    print('Test accuracy :', accuracy)\n",
    "    \n",
    "    return acc, val_acc, training_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(acc, val_acc, loss, val_loss):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test each of the 4 models from scratch. Comparing them by the testing accuracy and analyzing their Training and Validation Accuracy/Loss along the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, epochs:  20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmansilla/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7216/7216 [==============================] - 5033s 697ms/step - loss: 0.1653 - accuracy: 0.9324 - val_loss: 0.3136 - val_accuracy: 0.8085\n",
      "Epoch 2/20\n",
      "7216/7216 [==============================] - 4812s 667ms/step - loss: 0.1033 - accuracy: 0.9594 - val_loss: 0.1110 - val_accuracy: 0.9595\n",
      "Epoch 3/20\n",
      "7216/7216 [==============================] - 4785s 663ms/step - loss: 0.0906 - accuracy: 0.9645 - val_loss: 0.1095 - val_accuracy: 0.9521\n",
      "Epoch 4/20\n",
      "7216/7216 [==============================] - 4748s 658ms/step - loss: 0.0823 - accuracy: 0.9677 - val_loss: 0.1180 - val_accuracy: 0.9519\n",
      "Epoch 5/20\n",
      "7216/7216 [==============================] - 4722s 654ms/step - loss: 0.0759 - accuracy: 0.9702 - val_loss: 0.1109 - val_accuracy: 0.9495\n",
      "Epoch 6/20\n",
      "7216/7216 [==============================] - 4705s 652ms/step - loss: 0.0695 - accuracy: 0.9728 - val_loss: 0.1045 - val_accuracy: 0.9583\n",
      "Epoch 7/20\n",
      "7216/7216 [==============================] - 4691s 650ms/step - loss: 0.0640 - accuracy: 0.9747 - val_loss: 0.1303 - val_accuracy: 0.9413\n",
      "Epoch 8/20\n",
      "7216/7216 [==============================] - 4686s 649ms/step - loss: 0.0586 - accuracy: 0.9770 - val_loss: 0.1148 - val_accuracy: 0.9605\n",
      "Epoch 9/20\n",
      "7216/7216 [==============================] - 4675s 648ms/step - loss: 0.0542 - accuracy: 0.9786 - val_loss: 0.1186 - val_accuracy: 0.9549\n",
      "Epoch 10/20\n",
      "7216/7216 [==============================] - 4672s 647ms/step - loss: 0.0496 - accuracy: 0.9803 - val_loss: 0.1122 - val_accuracy: 0.9626\n",
      "Epoch 11/20\n",
      "7216/7216 [==============================] - 4652s 645ms/step - loss: 0.0457 - accuracy: 0.9818 - val_loss: 0.1381 - val_accuracy: 0.9500\n",
      "Epoch 12/20\n",
      "7216/7216 [==============================] - 4657s 645ms/step - loss: 0.0426 - accuracy: 0.9832 - val_loss: 0.1265 - val_accuracy: 0.9615\n",
      "Epoch 13/20\n",
      "7216/7216 [==============================] - 4651s 645ms/step - loss: 0.0399 - accuracy: 0.9843 - val_loss: 0.1531 - val_accuracy: 0.9458\n",
      "Epoch 14/20\n",
      "7216/7216 [==============================] - 4648s 644ms/step - loss: 0.0383 - accuracy: 0.9843 - val_loss: 0.1274 - val_accuracy: 0.9609\n",
      "Epoch 15/20\n",
      "7216/7216 [==============================] - 4646s 644ms/step - loss: 0.0357 - accuracy: 0.9859 - val_loss: 0.1545 - val_accuracy: 0.9484\n",
      "Epoch 16/20\n",
      "7216/7216 [==============================] - 4626s 641ms/step - loss: 0.0337 - accuracy: 0.9864 - val_loss: 0.1406 - val_accuracy: 0.9637\n",
      "Epoch 17/20\n",
      "5179/7216 [====================>.........] - ETA: 21:02 - loss: 0.0318 - accuracy: 0.9872"
     ]
    }
   ],
   "source": [
    "acc, val_acc, loss, val_loss = train_model(mobilenet_model, TRAINING_EPOCHS)\n",
    "show_plot(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, val_acc, loss, val_loss = train_model(densenet_model, TRAINING_EPOCHS)\n",
    "show_plot(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, val_acc, loss, val_loss = train_model(inception_resnet_model, TRAINING_EPOCHS)\n",
    "show_plot(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Training Accuracy | Testing Accuracy |\n",
    "| --- | --- | --- |\n",
    "| MobileNet | .9833 | .8997 |\n",
    "| DenseNet | .9921 | .9438 |\n",
    "| Inception Resnet | .9949 | .9383 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing them we choose MobileNet as it demands a smaller number of parameters to train. We use techniques in order to improve Training and Validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model again but with pre-set imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, val_acc, loss, val_loss = train_model(model, TRAINING_EPOCHS)\n",
    "show_plot(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve around 92% training accuracy and around 91% testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning without freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model again but with pre-set imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, val_acc, loss, val_loss = train_model(model, TRAINING_EPOCHS)\n",
    "show_plot(acc, val_acc, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve around 99.5% training accuracy and around 95% testing accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
